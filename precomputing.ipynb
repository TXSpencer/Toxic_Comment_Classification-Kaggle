{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRECOMPUTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the RoBERTa model and tokenizer\n",
    "ROBERTA_MODEL = 'roberta-large'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL)\n",
    "roberta_model = RobertaModel.from_pretrained(ROBERTA_MODEL).to(device)\n",
    "\n",
    "def clean(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = re.sub(\"\\\\n\", \" \", comment)\n",
    "    comment = re.sub(\"\\\\r\", \" \", comment)\n",
    "    return comment\n",
    "\n",
    "\n",
    "\n",
    "def precompute_embeddings(data_dir, mode, output_dir):\n",
    "    # Load data\n",
    "    data = pd.read_csv(os.path.join(data_dir, f'{mode}_x.csv'), index_col=0)\n",
    "    data.fillna(\"\", inplace=True)\n",
    "    data[\"string\"] = data[\"string\"].apply(lambda x: clean(x))\n",
    "\n",
    "    embeddings = []\n",
    "    labels = None\n",
    "\n",
    "    if mode != 'test':\n",
    "        labels = pd.read_csv(os.path.join(data_dir, f'{mode}_y.csv'))\n",
    "\n",
    "    # Compute embeddings\n",
    "    roberta_model.eval()\n",
    "    for text in tqdm(data[\"string\"].tolist(), desc=f\"Processing {mode}\"):\n",
    "        # Tokenize text\n",
    "        tokens = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens[\"input_ids\"].to(device)\n",
    "        attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Get RoBERTa embeddings\n",
    "        with torch.no_grad():\n",
    "            output = roberta_model(input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = output.last_hidden_state[:, 0, :]  # Extract [CLS] token\n",
    "\n",
    "        embeddings.append(cls_embedding.squeeze(0).cpu())\n",
    "\n",
    "    # Save embeddings and labels\n",
    "    embeddings_tensor = torch.stack(embeddings)\n",
    "    torch.save(embeddings_tensor, os.path.join(output_dir, f\"{mode}_embeddings.pt\"))\n",
    "\n",
    "    if labels is not None:\n",
    "        labels_tensor = torch.tensor(labels['y'].values)\n",
    "        torch.save(labels_tensor, os.path.join(output_dir, f\"{mode}_labels.pt\"))\n",
    "\n",
    "# Define paths\n",
    "data_dir = \"kaggle_data\"\n",
    "output_dir = \"embeddings/roberta\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Precompute for train, val, and test\n",
    "precompute_embeddings(data_dir, 'train', output_dir)\n",
    "precompute_embeddings(data_dir, 'val', output_dir)\n",
    "precompute_embeddings(data_dir, 'test', output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
