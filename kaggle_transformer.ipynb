{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'kaggle_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, tokenizer_name='bert-base-uncased', max_length=128):\n",
    "        super(TransformerDataset, self).__init__()\n",
    "        assert mode in ['train', 'val', 'test']\n",
    "        self.mode = mode\n",
    "\n",
    "        self.data = pd.read_csv(os.path.join(data_dir, f'{mode}_x.csv'), index_col=0)\n",
    "        self.data.fillna(\"\", inplace=True)\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            self.label = pd.read_csv(os.path.join(data_dir, f'{mode}_y.csv'))\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx, 0]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return input_ids, attention_mask, idx\n",
    "        else:\n",
    "            y = torch.tensor([self.label.iloc[idx, -2]]).float()\n",
    "            return input_ids, attention_mask, y, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "   \n",
    "    def __init__(self, model_name='bert-base-uncased', hidden_dim=768):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]  \n",
    "        x = self.dropout(hidden_state)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worst_group_accuracy(prediction, y):\n",
    "    \"\"\"\n",
    "        Compute the worst group accuracy, with the groups being defined by ['male', 'female', 'LGBTQ',\n",
    "        'christian', 'muslim', 'other_religions', 'black', 'white'] for positive and negative toxicity.\n",
    "        arguments:\n",
    "            prediction [pandas.DataFrame]: dataframe with 2 columns (index and pred)\n",
    "            y [pandas.DataFrame]: dataframe containing the metadata\n",
    "        returns:\n",
    "            wga [float]: worst group accuracy\n",
    "    \"\"\"\n",
    "    y.loc[prediction.index, 'pred'] = prediction.pred\n",
    "\n",
    "    categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n",
    "    accuracies = []\n",
    "    for category in categories:\n",
    "        for label in [0, 1]:\n",
    "            group = y.loc[y[category] == label]\n",
    "            group_accuracy = (group['y'] == (group['pred'] > 0.5)).mean()\n",
    "            accuracies.append(group_accuracy)\n",
    "    wga = np.min(accuracies)\n",
    "    return wga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "    losses, predictions, indices = [], [], []\n",
    "    for input_ids, attention_mask, y, idx in tqdm(dataloader, leave=False):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(input_ids, attention_mask)\n",
    "        loss = criterion(pred.squeeze(), y.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.extend([loss.item()] * len(y))\n",
    "        predictions.extend(pred.detach().cpu().squeeze().tolist())\n",
    "        indices.extend(idx.tolist())\n",
    "\n",
    "    pred_df = pd.DataFrame({'index': indices, 'pred': predictions})\n",
    "    dataset_loss = np.mean(losses)\n",
    "    dataset_metric = worst_group_accuracy(pred_df, y=dataloader.dataset.label)\n",
    "    return dataset_loss, dataset_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "epochs = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = TransformerDataset(data_dir, 'train')\n",
    "val_dataset = TransformerDataset(data_dir, 'val', tokenizer_name='bert-base-uncased')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = TransformerClassifier(model_name='bert-base-uncased').to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_metric = train_model(model, optimizer, criterion, train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Metric: {train_metric:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
